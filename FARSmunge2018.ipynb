{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FARSmunge2018.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gary-Schulke/Project3Startup/blob/master/FARSmunge2018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eoHs9lPtc6h",
        "colab_type": "code",
        "outputId": "91b28c20-6ea5-4f9f-8cca-bd4a7cd23a98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
        "\n",
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"ETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()\n",
        "import pandas as pd\n",
        "from pyspark import SparkFiles"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-25 01:33:19--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 914037 (893K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.9.jar.5’\n",
            "\n",
            "postgresql-42.2.9.j 100%[===================>] 892.61K  3.63MB/s    in 0.2s    \n",
            "\n",
            "2020-02-25 01:33:19 (3.63 MB/s) - ‘postgresql-42.2.9.jar.5’ saved [914037/914037]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTapHDG6thF_",
        "colab_type": "code",
        "outputId": "9094221a-e0ba-4339-ba13-c4b63cc4e4dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# Load in accident .csvs from S3 into a DataFrame\n",
        "accident2018url = \"https://project-3-data.s3.amazonaws.com/Cleaned/ACCIDENT2018.csv\"\n",
        "accident2017url = \"https://project-3-data.s3.amazonaws.com/Cleaned/ACCIDENT2017.csv\"\n",
        "accident2016url = \"https://project-3-data.s3.amazonaws.com/Cleaned/ACCIDENT2016.csv\"\n",
        "accident2015url = \"https://project-3-data.s3.amazonaws.com/Cleaned/ACCIDENT2015.csv\"\n",
        "accident2014url = \"https://project-3-data.s3.amazonaws.com/Cleaned/ACCIDENT2014.csv\"\n",
        "accident2013url = \"https://project-3-data.s3.amazonaws.com/Cleaned/ACCIDENT2013.csv\"\n",
        "accident2012url = \"https://project-3-data.s3.amazonaws.com/Cleaned/ACCIDENT2012.csv\"\n",
        "\n",
        "spark.sparkContext.addFile(accident2018url)\n",
        "spark.sparkContext.addFile(accident2017url)\n",
        "spark.sparkContext.addFile(accident2016url)\n",
        "spark.sparkContext.addFile(accident2015url)\n",
        "spark.sparkContext.addFile(accident2014url)\n",
        "spark.sparkContext.addFile(accident2013url)\n",
        "spark.sparkContext.addFile(accident2012url)\n",
        "\n",
        "accident2018 = spark.read.option('header', 'true').csv(SparkFiles.get(\"ACCIDENT2018.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "accident2017 = spark.read.option('header', 'true').csv(SparkFiles.get(\"ACCIDENT2017.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "accident2016 = spark.read.option('header', 'true').csv(SparkFiles.get(\"ACCIDENT2016.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "accident2015 = spark.read.option('header', 'true').csv(SparkFiles.get(\"ACCIDENT2015.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "accident2014 = spark.read.option('header', 'true').csv(SparkFiles.get(\"ACCIDENT2014.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "accident2013 = spark.read.option('header', 'true').csv(SparkFiles.get(\"ACCIDENT2013.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "accident2012 = spark.read.option('header', 'true').csv(SparkFiles.get(\"ACCIDENT2012.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "\n",
        "accident = accident2018.union(accident2017)\n",
        "accident = accident.union(accident2016)\n",
        "accident = accident.union(accident2015)\n",
        "accident = accident.union(accident2014)\n",
        "accident = accident.union(accident2013)\n",
        "accident = accident.union(accident2012)\n",
        "\n",
        "accident.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+---------+-----------+------------------+-----------+------------------+---------+----------+-------------+----------+----------+----------------+----+---+-----+----+----+------+----------------+\n",
            "|state|state_case|veh_crash|pedestrians|motorists_involved|rural_urban|          latitude|longitude|fatalities|drunk_drivers|state_name| city_name|   lighting_name|city|day|month|year|hour|minute|light_conditions|\n",
            "+-----+----------+---------+-----------+------------------+-----------+------------------+---------+----------+-------------+----------+----------+----------------+----+---+-----+----+----+------+----------------+\n",
            "|    1|     10001|        2|          0|                 1|          1|       33.59133056| -86.1319|         1|            0|        AL|   LINCOLN|            Dawn|1870|  5|    1|2018|   6|     0|               4|\n",
            "|    1|     10002|        1|          0|                 2|          2|       33.80918611| -87.2898|         2|            0|        AL|    JASPER|Dark Not Lighted|1780|  8|    1|2018|   0|    48|               2|\n",
            "|    1|     10003|        2|          0|                 2|          1|       32.76773611|  -86.564|         1|            0|        AL|   Unknown|Dark Not Lighted|   0|  8|    1|2018|  22|    50|               2|\n",
            "|    1|     10004|        1|          0|                 2|          1|       31.02780556| -87.6432|         1|            0|        AL|   Unknown|        Daylight|   0|  9|    1|2018|  13|     2|               1|\n",
            "|    1|     10005|        2|          0|                 2|          2|       33.33210278| -86.9938|         1|            0|        AL|  BESSEMER|        Daylight| 330| 19|    1|2018|   7|     9|               1|\n",
            "|    1|     10006|        2|          1|                 1|          1|       34.33426389| -85.8319|         1|            0|        AL|   Unknown|Dark Not Lighted|   0| 19|    1|2018|  22|     8|               2|\n",
            "|    1|     10007|        1|          1|                 1|          2|       30.68679444| -88.1268|         1|            0|        AL|    MOBILE|        Daylight|2100| 21|    1|2018|   9|    13|               1|\n",
            "|    1|     10008|        2|          1|                 1|          2|       33.47605556| -86.9264|         1|            0|        AL|   Unknown|Dark Not Lighted|   0| 23|    1|2018|   0|    32|               2|\n",
            "|    1|     10009|        1|          0|                 1|          2|       30.67824444| -88.1274|         1|            1|        AL|    MOBILE|    Dark Lighted|2100| 27|    1|2018|   0|    35|               3|\n",
            "|    1|     10010|        1|          1|                 1|          2|       33.41248333| -86.8066|         1|            0|        AL|   Unknown|Dark Not Lighted|   0| 28|    1|2018|  22|    15|               2|\n",
            "|    1|     10011|        3|          0|                 4|          1|       33.62233333|  -85.605|         1|            0|        AL|   Unknown|        Daylight|   0| 28|    1|2018|  12|     0|               1|\n",
            "|    1|     10012|        1|          0|                 1|          2|34.748000000000005| -86.7889|         1|            1|        AL|   Unknown|Dark Not Lighted|9997|  1|    1|2018|  22|    47|               2|\n",
            "|    1|     10013|        3|          0|                 4|          1|       34.69701944| -86.3776|         2|            0|        AL|    GURLEY|    Dark Lighted|1508|  1|    1|2018|  20|     5|               3|\n",
            "|    1|     10014|        2|          0|                 2|          1|       32.66133611| -87.5465|         1|            1|        AL|   Unknown|Dark Not Lighted|   0|  2|    1|2018|  21|     5|               2|\n",
            "|    1|     10015|        1|          0|                 2|          1|       33.79911389| -86.3405|         1|            0|        AL|   Unknown|        Daylight|   0|  2|    1|2018|  14|    30|               1|\n",
            "|    1|     10016|        1|          0|                 2|          1|       34.25033333| -86.8815|         1|            1|        AL|   Unknown|        Daylight|   0|  3|    1|2018|  16|    45|               1|\n",
            "|    1|     10017|        1|          1|                 1|          1|       33.92585833| -86.6046|         1|            0|        AL|   Unknown|        Daylight|   0|  4|    1|2018|  13|    22|               1|\n",
            "|    1|     10018|        2|          0|                 3|          1|       34.06671111| -86.7624|         1|            0|        AL|HANCEVILLE|            Dusk|1560|  4|    1|2018|  16|    44|               5|\n",
            "|    1|     10019|        1|          0|                 2|          2|       32.33431389| -86.2711|         1|            1|        AL|MONTGOMERY|    Dark Lighted|2130|  5|    1|2018|   2|     0|               3|\n",
            "|    1|     10020|        2|          0|                 2|          1|       34.68191389| -87.5908|         1|            0|        AL|   Unknown|        Daylight|   0|  5|    1|2018|   6|    10|               1|\n",
            "+-----+----------+---------+-----------+------------------+-----------+------------------+---------+----------+-------------+----------+----------+----------------+----+---+-----+----+----+------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU7fDH4zuiqi",
        "colab_type": "code",
        "outputId": "4518f0fc-da0c-4f9b-aa00-6e6b20157217",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "distract2018url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DISTRACT2018.csv\"\n",
        "distract2017url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DISTRACT2017.csv\"\n",
        "distract2016url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DISTRACT2016.csv\"\n",
        "distract2015url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DISTRACT2015.csv\"\n",
        "distract2014url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DISTRACT2014.csv\"\n",
        "distract2013url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DISTRACT2013.csv\"\n",
        "distract2012url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DISTRACT2012.csv\"\n",
        "\n",
        "spark.sparkContext.addFile(distract2018url)\n",
        "spark.sparkContext.addFile(distract2017url)\n",
        "spark.sparkContext.addFile(distract2016url)\n",
        "spark.sparkContext.addFile(distract2015url)\n",
        "spark.sparkContext.addFile(distract2014url)\n",
        "spark.sparkContext.addFile(distract2013url)\n",
        "spark.sparkContext.addFile(distract2012url)\n",
        "\n",
        "distract2018 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DISTRACT2018.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "distract2017 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DISTRACT2017.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "distract2016 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DISTRACT2016.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "distract2015 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DISTRACT2015.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "distract2014 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DISTRACT2014.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "distract2013 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DISTRACT2013.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "distract2012 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DISTRACT2012.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "\n",
        "distract = distract2018.union(distract2017)\n",
        "distract = distract.union(distract2016)\n",
        "distract = distract.union(distract2015)\n",
        "distract = distract.union(distract2014)\n",
        "distract = distract.union(distract2013)\n",
        "distract = distract.union(distract2012)\n",
        "\n",
        "distract.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+------------+--------------------+----------+----+\n",
            "|state|state_case|distract_num|       distract_desc|state_name|year|\n",
            "+-----+----------+------------+--------------------+----------+----+\n",
            "|    1|     10001|          96|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10002|          96|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10003|           0|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10003|           0|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10004|          96|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10005|          96|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10005|           0|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10006|           0|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10007|           0|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10008|           0|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10008|          16|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10009|          96|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10010|           0|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10011|          99|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10011|          96|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10011|          96|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10012|          96|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10013|          96|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10013|          96|Not/Unknown if Di...|        AL|2018|\n",
            "|    1|     10013|           0|Not/Unknown if Di...|        AL|2018|\n",
            "+-----+----------+------------+--------------------+----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvS7MY3G0EtQ",
        "colab_type": "code",
        "outputId": "c8fd0ec2-cb21-4865-ffd4-e23821782cd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "drimpair2018url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DRIMPAIR2018.csv\"\n",
        "drimpair2017url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DRIMPAIR2017.csv\"\n",
        "drimpair2016url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DRIMPAIR2016.csv\"\n",
        "drimpair2015url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DRIMPAIR2015.csv\"\n",
        "drimpair2014url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DRIMPAIR2014.csv\"\n",
        "drimpair2013url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DRIMPAIR2013.csv\"\n",
        "drimpair2012url = \"https://project-3-data.s3.amazonaws.com/Cleaned/DRIMPAIR2012.csv\"\n",
        "\n",
        "spark.sparkContext.addFile(drimpair2018url)\n",
        "spark.sparkContext.addFile(drimpair2017url)\n",
        "spark.sparkContext.addFile(drimpair2016url)\n",
        "spark.sparkContext.addFile(drimpair2015url)\n",
        "spark.sparkContext.addFile(drimpair2014url)\n",
        "spark.sparkContext.addFile(drimpair2013url)\n",
        "spark.sparkContext.addFile(drimpair2012url)\n",
        "\n",
        "drimpair2018 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DRIMPAIR2018.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "drimpair2017 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DRIMPAIR2017.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "drimpair2016 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DRIMPAIR2016.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "drimpair2015 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DRIMPAIR2015.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "drimpair2014 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DRIMPAIR2014.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "drimpair2013 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DRIMPAIR2013.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "drimpair2012 = spark.read.option('header', 'true').csv(SparkFiles.get(\"DRIMPAIR2012.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "\n",
        "drimpair = drimpair2018.union(drimpair2017)\n",
        "drimpair = drimpair.union(drimpair2016)\n",
        "drimpair = drimpair.union(drimpair2015)\n",
        "drimpair = drimpair.union(drimpair2014)\n",
        "drimpair = drimpair.union(drimpair2013)\n",
        "drimpair = drimpair.union(drimpair2012)\n",
        "\n",
        "drimpair.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+--------------+--------------------+----------+----+\n",
            "|state|state_case|imparement_num|     imparement_desc|state_name|year|\n",
            "+-----+----------+--------------+--------------------+----------+----+\n",
            "|    1|     10001|             0|                None|        AL|2018|\n",
            "|    1|     10002|            99|             Unknown|        AL|2018|\n",
            "|    1|     10003|            99|             Unknown|        AL|2018|\n",
            "|    1|     10003|             0|                None|        AL|2018|\n",
            "|    1|     10004|            99|             Unknown|        AL|2018|\n",
            "|    1|     10005|            99|             Unknown|        AL|2018|\n",
            "|    1|     10005|             0|                None|        AL|2018|\n",
            "|    1|     10006|             0|                None|        AL|2018|\n",
            "|    1|     10007|             0|                None|        AL|2018|\n",
            "|    1|     10008|             0|                None|        AL|2018|\n",
            "|    1|     10008|            95|             Unknown|        AL|2018|\n",
            "|    1|     10009|            99|             Unknown|        AL|2018|\n",
            "|    1|     10010|             0|                None|        AL|2018|\n",
            "|    1|     10011|            99|             Unknown|        AL|2018|\n",
            "|    1|     10011|             0|                None|        AL|2018|\n",
            "|    1|     10011|             0|                None|        AL|2018|\n",
            "|    1|     10012|             9|Legal and Illegal...|        AL|2018|\n",
            "|    1|     10013|             0|                None|        AL|2018|\n",
            "|    1|     10013|             0|                None|        AL|2018|\n",
            "|    1|     10013|             0|                None|        AL|2018|\n",
            "+-----+----------+--------------+--------------------+----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q2AiKHDH4qi",
        "colab_type": "code",
        "outputId": "64ac9cd0-e650-4f71-d888-bdb19990084d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "person2018url = \"https://project-3-data.s3.amazonaws.com/Cleaned/UpdatedPerson/PERSON2018.csv\"\n",
        "person2017url = \"https://project-3-data.s3.amazonaws.com/Cleaned/UpdatedPerson/PERSON2017.csv\"\n",
        "person2016url = \"https://project-3-data.s3.amazonaws.com/Cleaned/UpdatedPerson/PERSON2016.csv\"\n",
        "person2015url = \"https://project-3-data.s3.amazonaws.com/Cleaned/UpdatedPerson/PERSON2015.csv\"\n",
        "person2014url = \"https://project-3-data.s3.amazonaws.com/Cleaned/UpdatedPerson/PERSON2014.csv\"\n",
        "person2013url = \"https://project-3-data.s3.amazonaws.com/Cleaned/UpdatedPerson/PERSON2013.csv\"\n",
        "person2012url = \"https://project-3-data.s3.amazonaws.com/Cleaned/UpdatedPerson/PERSON2012.csv\"\n",
        "\n",
        "spark.sparkContext.addFile(person2018url)\n",
        "spark.sparkContext.addFile(person2017url)\n",
        "spark.sparkContext.addFile(person2016url)\n",
        "spark.sparkContext.addFile(person2015url)\n",
        "spark.sparkContext.addFile(person2014url)\n",
        "spark.sparkContext.addFile(person2013url)\n",
        "spark.sparkContext.addFile(person2012url)\n",
        "\n",
        "person2018 = spark.read.option('header', 'true').csv(SparkFiles.get(\"PERSON2018.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "person2017 = spark.read.option('header', 'true').csv(SparkFiles.get(\"PERSON2017.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "person2016 = spark.read.option('header', 'true').csv(SparkFiles.get(\"PERSON2016.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "person2015 = spark.read.option('header', 'true').csv(SparkFiles.get(\"PERSON2015.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "person2014 = spark.read.option('header', 'true').csv(SparkFiles.get(\"PERSON2014.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "person2013 = spark.read.option('header', 'true').csv(SparkFiles.get(\"PERSON2013.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "person2012 = spark.read.option('header', 'true').csv(SparkFiles.get(\"PERSON2012.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "\n",
        "person = person2018.union(person2017)\n",
        "person = person.union(person2016)\n",
        "person = person.union(person2015)\n",
        "person = person.union(person2014)\n",
        "person = person.union(person2013)\n",
        "person = person.union(person2012)\n",
        "\n",
        "person.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+----------+---+---+---------------+------------+---------------+-----+--------+---+----------+------------+----+\n",
            "|state|state_case|person_num|age|sex|injury_severity|alc_involved|alc_test_result|drugs|drug_num|doa|state_name|   drug_desc|year|\n",
            "+-----+----------+----------+---+---+---------------+------------+---------------+-----+--------+---+----------+------------+----+\n",
            "|    1|     10001|         1| 55|  1|              4|           0|              0|    0|       1|  7|        AL|    No Drugs|2018|\n",
            "|    1|     10002|         1| 24|  1|              4|           9|            996|    9|       0|  7|        AL|Not Reported|2018|\n",
            "|    1|     10002|         2| 24|  1|              4|           8|            996|    8|       0|  7|        AL|Not Reported|2018|\n",
            "|    1|     10003|         1| 37|  2|              4|           0|              0|    9|     401|  7|        AL|   Stimulant|2018|\n",
            "|    1|     10003|         1| 36|  1|              2|           0|            996|    0|       0|  0|        AL|Not Reported|2018|\n",
            "|    1|     10004|         1| 58|  1|              0|           9|              0|    9|       1|  0|        AL|    No Drugs|2018|\n",
            "|    1|     10004|         2| 26|  2|              4|           8|              0|    8|       1|  7|        AL|    No Drugs|2018|\n",
            "|    1|     10005|         1| 28|  1|              4|           9|            996|    0|       0|  7|        AL|Not Reported|2018|\n",
            "|    1|     10005|         1| 46|  1|              2|           0|            996|    0|       0|  0|        AL|Not Reported|2018|\n",
            "|    1|     10006|         1| 38|  1|              4|           0|            996|    0|       0|  7|        AL|Not Reported|2018|\n",
            "|    1|     10006|         1| 23|  1|              0|           0|              0|    0|       1|  0|        AL|    No Drugs|2018|\n",
            "|    1|     10006|         1| 42|  2|              3|           8|            996|    8|       0|  0|        AL|Not Reported|2018|\n",
            "|    1|     10006|         2|  6|  1|              3|           8|            996|    8|       0|  0|        AL|Not Reported|2018|\n",
            "|    1|     10006|         3|  5|  1|              3|           8|            996|    8|       0|  0|        AL|Not Reported|2018|\n",
            "|    1|     10007|         1| 19|  2|              4|           0|              0|    0|       1|  0|        AL|    No Drugs|2018|\n",
            "|    1|     10007|         1| 73|  1|              0|           0|            996|    0|       0|  0|        AL|Not Reported|2018|\n",
            "|    1|     10008|         1| 23|  1|              4|           9|            996|    9|       0|  0|        AL|Not Reported|2018|\n",
            "|    1|     10008|         1| 40|  1|              0|           0|            996|    0|       0|  0|        AL|Not Reported|2018|\n",
            "|    1|     10009|         1| 49|  1|              4|           9|            209|    9|       1|  7|        AL|    No Drugs|2018|\n",
            "|    1|     10010|         1| 46|  1|              4|           9|            173|    9|     407|  7|        AL|   Stimulant|2018|\n",
            "+-----+----------+----------+---+---+---------------+------------+---------------+-----+--------+---+----------+------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gky1o90RLIfV",
        "colab_type": "code",
        "outputId": "73770cce-204e-42e0-bcec-000f89f5cdde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "violatn2018url = \"https://project-3-data.s3.amazonaws.com/Cleaned/VIOLATN2018.csv\"\n",
        "violatn2017url = \"https://project-3-data.s3.amazonaws.com/Cleaned/VIOLATN2017.csv\"\n",
        "violatn2016url = \"https://project-3-data.s3.amazonaws.com/Cleaned/VIOLATN2016.csv\"\n",
        "violatn2015url = \"https://project-3-data.s3.amazonaws.com/Cleaned/VIOLATN2015.csv\"\n",
        "violatn2014url = \"https://project-3-data.s3.amazonaws.com/Cleaned/VIOLATN2014.csv\"\n",
        "violatn2013url = \"https://project-3-data.s3.amazonaws.com/Cleaned/VIOLATN2013.csv\"\n",
        "violatn2012url = \"https://project-3-data.s3.amazonaws.com/Cleaned/VIOLATN2012.csv\"\n",
        "\n",
        "spark.sparkContext.addFile(violatn2018url)\n",
        "spark.sparkContext.addFile(violatn2017url)\n",
        "spark.sparkContext.addFile(violatn2016url)\n",
        "spark.sparkContext.addFile(violatn2015url)\n",
        "spark.sparkContext.addFile(violatn2014url)\n",
        "spark.sparkContext.addFile(violatn2013url)\n",
        "spark.sparkContext.addFile(violatn2012url)\n",
        "\n",
        "violatn2018 = spark.read.option('header', 'true').csv(SparkFiles.get(\"VIOLATN2018.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "violatn2017 = spark.read.option('header', 'true').csv(SparkFiles.get(\"VIOLATN2017.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "violatn2016 = spark.read.option('header', 'true').csv(SparkFiles.get(\"VIOLATN2016.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "violatn2015 = spark.read.option('header', 'true').csv(SparkFiles.get(\"VIOLATN2015.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "violatn2014 = spark.read.option('header', 'true').csv(SparkFiles.get(\"VIOLATN2014.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "violatn2013 = spark.read.option('header', 'true').csv(SparkFiles.get(\"VIOLATN2013.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "violatn2012 = spark.read.option('header', 'true').csv(SparkFiles.get(\"VIOLATN2012.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "\n",
        "violation = violatn2018.union(violatn2017)\n",
        "violation = violation.union(violatn2016)\n",
        "violation = violation.union(violatn2015)\n",
        "violation = violation.union(violatn2014)\n",
        "violation = violation.union(violatn2013)\n",
        "violation = violation.union(violatn2012)\n",
        "\n",
        "violation.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+-------------+------------------+----------+----+\n",
            "|state|state_case|violation_num|    violation_desc|state_name|year|\n",
            "+-----+----------+-------------+------------------+----------+----+\n",
            "|    1|     10001|            0|              None|        AL|2018|\n",
            "|    1|     10002|            0|              None|        AL|2018|\n",
            "|    1|     10003|            0|              None|        AL|2018|\n",
            "|    1|     10003|            0|              None|        AL|2018|\n",
            "|    1|     10004|            0|              None|        AL|2018|\n",
            "|    1|     10005|            0|              None|        AL|2018|\n",
            "|    1|     10005|            0|              None|        AL|2018|\n",
            "|    1|     10006|            0|              None|        AL|2018|\n",
            "|    1|     10007|            0|              None|        AL|2018|\n",
            "|    1|     10008|            0|              None|        AL|2018|\n",
            "|    1|     10008|           95|License Violations|        AL|2018|\n",
            "|    1|     10009|            0|              None|        AL|2018|\n",
            "|    1|     10010|            0|              None|        AL|2018|\n",
            "|    1|     10011|            0|              None|        AL|2018|\n",
            "|    1|     10011|            0|              None|        AL|2018|\n",
            "|    1|     10011|            0|              None|        AL|2018|\n",
            "|    1|     10012|            0|              None|        AL|2018|\n",
            "|    1|     10013|            0|              None|        AL|2018|\n",
            "|    1|     10013|            0|              None|        AL|2018|\n",
            "|    1|     10013|            0|              None|        AL|2018|\n",
            "+-----+----------+-------------+------------------+----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRa5N-XgMha4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mode=\"append\"\n",
        "jdbc_url = \"jdbc:postgresql://myprojectdb.cqbbih9hnlbh.us-east-1.rds.amazonaws.com:5432/root\"\n",
        "config = {\"user\":\"root\",\n",
        "          \"password\": \"finalproject\",\n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx03WL7rZTi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accident.write.jdbc(url=jdbc_url, table='accident', mode=mode, properties=config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMbEO6J7bV3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distract.write.jdbc(url=jdbc_url, table='distract', mode=mode, properties=config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAZdv_wrFWpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drimpair.write.jdbc(url=jdbc_url, table='drimpair', mode=mode, properties=config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eaO_SfLHQoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "person.write.jdbc(url=jdbc_url, table='person', mode=mode, properties=config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biuQnnGeJN5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "violation.write.jdbc(url=jdbc_url, table='violation', mode=mode, properties=config)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}